# -*- coding: utf-8 -*-
"""mmlu.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/knc6/33a34ac2065cce34759717a655563850/mmlu.ipynb
"""

!pip install -q transformers jarvis-tools

!rm *.json *.zip
!wget https://figshare.com/ndownloader/files/44211497 -O mmlu_test.zip

!ls

!unzip mmlu_test.zip

from jarvis.db.jsonutils import loadjson
d = loadjson('mmlu_test.json')

d[0]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import argparse
# import os
# import torch
# import numpy as np
# import pandas as pd
# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer,AutoModelForCausalLM
# import time
# from tqdm import tqdm
# model_name = "meta-llama/Llama-2-13b-chat-hf"
# model_name = "allenai/unifiedqa-v2-t5-small-1363200"
# model_name = "openai-community/gpt2-large"
# model_name = "mistralai/Mistral-7B-v0.1"
# model_name = "facebook/opt-350m"
# model_name = "facebook/opt-2.7b"
# model_name = "google/flan-t5-small"
# 
# 
# 
# 
# if 't5' in model_name  :
#     model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
#     tokenizer = AutoTokenizer.from_pretrained(model_name)
# 
# 
# if 't5' not in model_name:
#     model = AutoModelForCausalLM.from_pretrained(model_name)
#     tokenizer = AutoTokenizer.from_pretrained(model_name,use_fast=False)
# f= open('mmlu_test_results.csv','w')
# f.write('id,target,prediction\n')
# target_labels=[]
# pred_labels=[]
# for ii, i in enumerate(tqdm(d)):
#   prompt = i['prompt']
#   label = i['answer']
#   input_ids = tokenizer(prompt, return_tensors="pt").input_ids
#   decoder_input_ids = tokenizer("", return_tensors="pt").input_ids #.cuda()
#   decoder_input_ids = model._shift_right(decoder_input_ids)
#   logits = model(
#       input_ids=input_ids, decoder_input_ids=decoder_input_ids
#   ).logits.flatten()
# 
#   #logits = model(input_ids=input_ids).logits.flatten()
#   probs = (
#       torch.nn.functional.softmax(
#           torch.tensor(
#               [
#                   logits[tokenizer("A").input_ids[0]],
#                   logits[tokenizer("B").input_ids[0]],
#                   logits[tokenizer("C").input_ids[0]],
#                   logits[tokenizer("D").input_ids[0]],
#               ]
#           ),
#           dim=0,
#       )
#       .detach()
#       .cpu()
#       .numpy()
#   )
#   pred = {0: "A", 1: "B", 2: "C", 3: "D"}[np.argmax(probs)]
#   # print("prompt",prompt)
#   # print("label",label)
#   # print("pred",pred)
#   # print()
#   target_labels.append(label)
#   pred_labels.append(pred)
#   line=i['id']+','+label+','+pred+'\n'
#   #print(line)
#   f.write(line)
# 
# f.close()
#

from jarvis.db.jsonutils import loadjson, dumpjson
mem={}
mem['train']={}
tmp={}
for ii, i in enumerate(tqdm(d)):
  tmp[i['id']]=i['answer']
mem['test']=tmp
fname='mmlu_test_quiz.json'
dumpjson(data=mem,filename=fname)

!zip mmlu_test_quiz.json.zip mmlu_test_quiz.json

!cp mmlu_test_results.csv AI-TextClass-quiz-mmlu_test-test-acc.csv

!zip AI-TextClass-quiz-mmlu_test-test-acc.csv.zip AI-TextClass-quiz-mmlu_test-test-acc.csv

len(target_labels),len(pred_labels)

from sklearn.metrics import accuracy_score
accuracy_score(target_labels,pred_labels)













!wget https://people.eecs.berkeley.edu/~hendrycks/data.tar
!tar -xvf data.tar

d[0]





"""# Prepare dataset"""

# Commented out IPython magic to ensure Python compatibility.
# !wget https://people.eecs.berkeley.edu/~hendrycks/data.tar
# !tar -xvf data.tar
# import argparse
# import os
# import torch
# import numpy as np
# import pandas as pd
# # from categories import subcategories, categories
# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer,AutoModelForCausalLM
# import time
# 
# choices = ["A", "B", "C", "D"]
# data_dir = 'data'
# 
# 
# subcategories = {
#     "abstract_algebra": ["math"],
#     "anatomy": ["health"],
#     "astronomy": ["physics"],
#     "business_ethics": ["business"],
#     "clinical_knowledge": ["health"],
#     "college_biology": ["biology"],
#     "college_chemistry": ["chemistry"],
#     "college_computer_science": ["computer science"],
#     "college_mathematics": ["math"],
#     "college_medicine": ["health"],
#     "college_physics": ["physics"],
#     "computer_security": ["computer science"],
#     "conceptual_physics": ["physics"],
#     "econometrics": ["economics"],
#     "electrical_engineering": ["engineering"],
#     "elementary_mathematics": ["math"],
#     "formal_logic": ["philosophy"],
#     "global_facts": ["other"],
#     "high_school_biology": ["biology"],
#     "high_school_chemistry": ["chemistry"],
#     "high_school_computer_science": ["computer science"],
#     "high_school_european_history": ["history"],
#     "high_school_geography": ["geography"],
#     "high_school_government_and_politics": ["politics"],
#     "high_school_macroeconomics": ["economics"],
#     "high_school_mathematics": ["math"],
#     "high_school_microeconomics": ["economics"],
#     "high_school_physics": ["physics"],
#     "high_school_psychology": ["psychology"],
#     "high_school_statistics": ["math"],
#     "high_school_us_history": ["history"],
#     "high_school_world_history": ["history"],
#     "human_aging": ["health"],
#     "human_sexuality": ["culture"],
#     "international_law": ["law"],
#     "jurisprudence": ["law"],
#     "logical_fallacies": ["philosophy"],
#     "machine_learning": ["computer science"],
#     "management": ["business"],
#     "marketing": ["business"],
#     "medical_genetics": ["health"],
#     "miscellaneous": ["other"],
#     "moral_disputes": ["philosophy"],
#     "moral_scenarios": ["philosophy"],
#     "nutrition": ["health"],
#     "philosophy": ["philosophy"],
#     "prehistory": ["history"],
#     "professional_accounting": ["other"],
#     "professional_law": ["law"],
#     "professional_medicine": ["health"],
#     "professional_psychology": ["psychology"],
#     "public_relations": ["politics"],
#     "security_studies": ["politics"],
#     "sociology": ["culture"],
#     "us_foreign_policy": ["politics"],
#     "virology": ["health"],
#     "world_religions": ["philosophy"],
# }
# 
# categories = {
#     "STEM": ["physics", "chemistry", "biology", "computer science", "math", "engineering"],
#     "humanities": ["history", "philosophy", "law"],
#     "social sciences": ["politics", "culture", "economics", "geography", "psychology"],
#     "other (business, health, misc.)": ["other", "business", "health"],
# }
# #########
# # https://github.com/FlagOpen/FlagPerf/tree/13699a0e188fd9b1d2bb02ae97a2bcaba037a149/training/benchmarks
# # https://github.com/FranxYao/chain-of-thought-hub/tree/main/MMLU
# # https://github.com/THUDM/AgentTuning/blob/e33a45d7eab2b63cac4d1956da1e6377fca9fcc7/eval_general/eval_mmlu_hf.py#L31
# # https://medium.com/@indirakrigan/lessons-i-learned-while-i-arrived-at-llama-2-the-long-way-4a9a0c903bf
# 
# %%time
# 
# from huggingface_hub import login
# 
# 
# def format_example(df, idx, include_answer=False):
#     prompt = df.iloc[idx, 0]
#     k = df.shape[1] - 2
#     for j in range(k):
#         prompt += " {}. {}".format(choices[j], df.iloc[idx, j + 1])
#     prompt += " Answer:"
#     if include_answer:
#         prompt += " {}  ".format(df.iloc[idx, k + 1])
#     return prompt
# 
# #model_name = "google/flan-t5-small"
# #model_name = "google/flan-t5-xl"
# #model_name = "openai-community/gpt2-medium"
# model_name = "meta-llama/Llama-2-13b-chat-hf"
# model_name = "allenai/unifiedqa-v2-t5-small-1363200"
# model_name = "openai-community/gpt2-large"
# model_name = "mistralai/Mistral-7B-v0.1"
# model_name = "facebook/opt-350m"
# model_name = "facebook/opt-2.7b"
# model_name = "google/flan-t5-small"
# 
# from huggingface_hub import snapshot_download
# token = "hf_XXX"
# 
# #path = snapshot_download(repo_id="meta-llama/Llama-2-7b",cache_dir="Llama-2-7b", use_auth_token=token)
# 
# 
# if 't5' in model_name  :
#     model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
#     tokenizer = AutoTokenizer.from_pretrained(model_name)
# 
# 
# if 't5' not in model_name:
#     model = AutoModelForCausalLM.from_pretrained(model_name)
#     tokenizer = AutoTokenizer.from_pretrained(model_name,use_fast=False)
# 
# subjects = []
# all_resp=[]
# true_resp=[]
# count=0
# mem=[]
# for cat in list(categories.keys()):
#  for f in os.listdir(os.path.join(data_dir, "test")):
#     if "_test.csv" in f :
#         tmp = f.split("_test.csv")[0]
#         if len(set(subcategories[tmp]).intersection(categories[cat]))>0:
# 
# 
#             subject = tmp
#             subjects.append(subject)
#             test_df = pd.read_csv(os.path.join(data_dir, "test", subject + "_test.csv"), header=None)
#             for i,ii in test_df.iterrows():
#                 prompt = format_example(test_df,i)
#                 count+=1
# 
#                 id = cat+'-'+(subcategories[tmp][0])+'-'+tmp+'-'+str(count)
#                 #print('tmp',tmp,subcategories[tmp],id,prompt,ii.iloc[-1])
#                 info={}
#                 info['id'] = id
#                 info['answer'] = ii.iloc[-1]
#                 info['prompt'] = prompt
#                 mem.append(info)
#                 if 't5' in model_name :
#                     model.to('cpu')
#                     model.eval()
# 
#                 input_ids = tokenizer(prompt, return_tensors="pt").input_ids #.cuda()
# 
#                 # while input_ids.shape[-1] > 2048:
#                 #     k -= 1
#                 #     train_prompt = gen_prompt(dev_df, subject, k)
#                 #     prompt = train_prompt + prompt_end
#                 input_ids = tokenizer(prompt, return_tensors="pt").input_ids #.cuda()
# 
#                 label = test_df.iloc[i, test_df.shape[1] - 1]
#                 if 't5' not in model_name  :# == "openai-community/gpt2-medium":
# 
# 
#                     logits = model(
#                         input_ids=input_ids
#                     ).logits.flatten()
#                 if 't5' in model_name :
#                     decoder_input_ids = tokenizer("", return_tensors="pt").input_ids #.cuda()
#                     decoder_input_ids = model._shift_right(decoder_input_ids)
#                     logits = model(
#                        input_ids=input_ids, decoder_input_ids=decoder_input_ids
#                     ).logits.flatten()
# 
# 
#                 probs = (
#                     torch.nn.functional.softmax(
#                         torch.tensor(
#                             [
#                                 logits[tokenizer("A").input_ids[0]],
#                                 logits[tokenizer("B").input_ids[0]],
#                                 logits[tokenizer("C").input_ids[0]],
#                                 logits[tokenizer("D").input_ids[0]],
#                             ]
#                         ),
#                         dim=0,
#                     )
#                     .detach()
#                     .cpu()
#                     .numpy()
#                 )
#                 pred = {0: "A", 1: "B", 2: "C", 3: "D"}[np.argmax(probs)]
#                 label = ii.iloc[-1]
#                 #print('prompt',prompt,label,pred)
#                 #print()
#                 all_resp.append(label)
#                 if label==pred:
#                     true_resp.append(label)
# 
# 
# print(model_name,len(true_resp)/len(all_resp))
# from jarvis.db.jsonutils import dumpjson
# dumpjson(filename='mmlu_test.json',data=mem)
# !zip mmlu_test.zip mmlu_test.json

import argparse
import os
import torch
import numpy as np
import pandas as pd
# from categories import subcategories, categories
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer,AutoModelForCausalLM
import time

choices = ["A", "B", "C", "D"]
data_dir = 'data'


subcategories = {
    "abstract_algebra": ["math"],
    "anatomy": ["health"],
    "astronomy": ["physics"],
    "business_ethics": ["business"],
    "clinical_knowledge": ["health"],
    "college_biology": ["biology"],
    "college_chemistry": ["chemistry"],
    "college_computer_science": ["computer science"],
    "college_mathematics": ["math"],
    "college_medicine": ["health"],
    "college_physics": ["physics"],
    "computer_security": ["computer science"],
    "conceptual_physics": ["physics"],
    "econometrics": ["economics"],
    "electrical_engineering": ["engineering"],
    "elementary_mathematics": ["math"],
    "formal_logic": ["philosophy"],
    "global_facts": ["other"],
    "high_school_biology": ["biology"],
    "high_school_chemistry": ["chemistry"],
    "high_school_computer_science": ["computer science"],
    "high_school_european_history": ["history"],
    "high_school_geography": ["geography"],
    "high_school_government_and_politics": ["politics"],
    "high_school_macroeconomics": ["economics"],
    "high_school_mathematics": ["math"],
    "high_school_microeconomics": ["economics"],
    "high_school_physics": ["physics"],
    "high_school_psychology": ["psychology"],
    "high_school_statistics": ["math"],
    "high_school_us_history": ["history"],
    "high_school_world_history": ["history"],
    "human_aging": ["health"],
    "human_sexuality": ["culture"],
    "international_law": ["law"],
    "jurisprudence": ["law"],
    "logical_fallacies": ["philosophy"],
    "machine_learning": ["computer science"],
    "management": ["business"],
    "marketing": ["business"],
    "medical_genetics": ["health"],
    "miscellaneous": ["other"],
    "moral_disputes": ["philosophy"],
    "moral_scenarios": ["philosophy"],
    "nutrition": ["health"],
    "philosophy": ["philosophy"],
    "prehistory": ["history"],
    "professional_accounting": ["other"],
    "professional_law": ["law"],
    "professional_medicine": ["health"],
    "professional_psychology": ["psychology"],
    "public_relations": ["politics"],
    "security_studies": ["politics"],
    "sociology": ["culture"],
    "us_foreign_policy": ["politics"],
    "virology": ["health"],
    "world_religions": ["philosophy"],
}

categories = {
    "STEM": ["physics", "chemistry", "biology", "computer science", "math", "engineering"],
    "humanities": ["history", "philosophy", "law"],
    "social sciences": ["politics", "culture", "economics", "geography", "psychology"],
    "other (business, health, misc.)": ["other", "business", "health"],
}

pip install sentencepiece

# Commented out IPython magic to ensure Python compatibility.
# # https://github.com/FlagOpen/FlagPerf/tree/13699a0e188fd9b1d2bb02ae97a2bcaba037a149/training/benchmarks
# # https://github.com/FranxYao/chain-of-thought-hub/tree/main/MMLU
# # https://github.com/THUDM/AgentTuning/blob/e33a45d7eab2b63cac4d1956da1e6377fca9fcc7/eval_general/eval_mmlu_hf.py#L31
# # https://medium.com/@indirakrigan/lessons-i-learned-while-i-arrived-at-llama-2-the-long-way-4a9a0c903bf
# 
# %%time
# 
# from huggingface_hub import login
# 
# 
# def format_example(df, idx, include_answer=False):
#     prompt = df.iloc[idx, 0]
#     k = df.shape[1] - 2
#     for j in range(k):
#         prompt += " {}. {}".format(choices[j], df.iloc[idx, j + 1])
#     prompt += " Answer:"
#     if include_answer:
#         prompt += " {}  ".format(df.iloc[idx, k + 1])
#     return prompt
# 
# #model_name = "google/flan-t5-small"
# #model_name = "google/flan-t5-xl"
# #model_name = "openai-community/gpt2-medium"
# model_name = "meta-llama/Llama-2-13b-chat-hf"
# model_name = "allenai/unifiedqa-v2-t5-small-1363200"
# model_name = "openai-community/gpt2-large"
# model_name = "mistralai/Mistral-7B-v0.1"
# model_name = "facebook/opt-350m"
# model_name = "facebook/opt-2.7b"
# model_name = "google/flan-t5-small"
# 
# from huggingface_hub import snapshot_download
# token = "hf_XXX"
# 
# #path = snapshot_download(repo_id="meta-llama/Llama-2-7b",cache_dir="Llama-2-7b", use_auth_token=token)
# 
# 
# if 't5' in model_name  :
#     model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
#     tokenizer = AutoTokenizer.from_pretrained(model_name)
# 
# 
# if 't5' not in model_name:
#     model = AutoModelForCausalLM.from_pretrained(model_name)
#     tokenizer = AutoTokenizer.from_pretrained(model_name,use_fast=False)
# 
# subjects = []
# all_resp=[]
# true_resp=[]
# count=0
# mem=[]
# for cat in list(categories.keys()):
#  for f in os.listdir(os.path.join(data_dir, "test")):
#     if "_test.csv" in f :
#         tmp = f.split("_test.csv")[0]
#         if len(set(subcategories[tmp]).intersection(categories[cat]))>0:
# 
# 
#             subject = tmp
#             subjects.append(subject)
#             test_df = pd.read_csv(os.path.join(data_dir, "test", subject + "_test.csv"), header=None)
#             for i,ii in test_df.iterrows():
#                 prompt = format_example(test_df,i)
#                 count+=1
# 
#                 id = cat+'-'+(subcategories[tmp][0])+'-'+tmp+'-'+str(count)
#                 #print('tmp',tmp,subcategories[tmp],id,prompt,ii.iloc[-1])
#                 info={}
#                 info['id'] = id
#                 info['answer'] = ii.iloc[-1]
#                 info['prompt'] = prompt
#                 mem.append(info)
#                 if 't5' in model_name :
#                     model.to('cpu')
#                     model.eval()
# 
#                 input_ids = tokenizer(prompt, return_tensors="pt").input_ids #.cuda()
# 
#                 # while input_ids.shape[-1] > 2048:
#                 #     k -= 1
#                 #     train_prompt = gen_prompt(dev_df, subject, k)
#                 #     prompt = train_prompt + prompt_end
#                 input_ids = tokenizer(prompt, return_tensors="pt").input_ids #.cuda()
# 
#                 label = test_df.iloc[i, test_df.shape[1] - 1]
#                 if 't5' not in model_name  :# == "openai-community/gpt2-medium":
# 
# 
#                     logits = model(
#                         input_ids=input_ids
#                     ).logits.flatten()
#                 if 't5' in model_name :
#                     decoder_input_ids = tokenizer("", return_tensors="pt").input_ids #.cuda()
#                     decoder_input_ids = model._shift_right(decoder_input_ids)
#                     logits = model(
#                        input_ids=input_ids, decoder_input_ids=decoder_input_ids
#                     ).logits.flatten()
# 
# 
#                 probs = (
#                     torch.nn.functional.softmax(
#                         torch.tensor(
#                             [
#                                 logits[tokenizer("A").input_ids[0]],
#                                 logits[tokenizer("B").input_ids[0]],
#                                 logits[tokenizer("C").input_ids[0]],
#                                 logits[tokenizer("D").input_ids[0]],
#                             ]
#                         ),
#                         dim=0,
#                     )
#                     .detach()
#                     .cpu()
#                     .numpy()
#                 )
#                 pred = {0: "A", 1: "B", 2: "C", 3: "D"}[np.argmax(probs)]
#                 label = ii.iloc[-1]
#                 #print('prompt',prompt,label,pred)
#                 #print()
#                 all_resp.append(label)
#                 if label==pred:
#                     true_resp.append(label)
# 
# 
# print(model_name,len(true_resp)/len(all_resp))

mem[0]

len(mem)

from jarvis.db.jsonutils import dumpjson
dumpjson(filename='mmlu_test.json',data=mem)
!zip mmlu_test.zip mmlu_test.json

!zip mmlu_test.zip mmlu_test.json

#google/flan-t5-small 0.26938369781312127
#google/flan-t5-xl 0.38601722995361165
#openai-community/gpt2-medium 0.2756792577866137
#openai-community/gpt2-large 0.25281643472498344
#facebook/opt-350m 0.21371769383697814
#facebook/opt-2.7b 0.21371769383697814